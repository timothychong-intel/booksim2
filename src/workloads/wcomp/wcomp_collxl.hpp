/*
 * workload component for modeling a hardware collectives accelerator
 *
 * C. Beckmann (c) Intel, 2023
 *
 */
#pragma once

#include "wkld_comp.hpp"
#include <exception>
#include <vector>
#include <deque>
#include <boost/coroutine/all.hpp>

//////////////////
// Collectives CollectiveAccel Layer - traffic modifier
class CollectiveAccel : public WComp<CollectiveAccel>, private GeneratorWorkloadMessage::Factory
{
  public:
    enum collx_op_t  { CX_NO_OP, CX_BARRIER, CX_ALLREDUCE, CX_BCAST };

    // accelerator request/reply messages from/to compute PEs
    class Request : public GeneratorWorkloadMessage
    {
      public:
        const collx_op_t operation;
        const int num_pes;
        const int count;
        const int type_size; 

        Request(GeneratorWorkloadMessage::Factory *f, int src, int num, collx_op_t op = CX_BARRIER, int count = 0, int tsize = 8)
          : GeneratorWorkloadMessage(f, src, -1, DummyRequest, false, 0, count*tsize),
            operation(op), num_pes(num), count(count), type_size(tsize) {}
        Request(const Request &r, bool make_reply = false) // copy- and reply- constructor
          : GeneratorWorkloadMessage(r, make_reply), operation(r.operation), num_pes(r.num_pes), count(r.count), type_size(r.type_size) {}

        WorkloadMessagePtr Reply() { return new Request(*this, true); }
        bool IsBlocking() const { return operation == CX_BARRIER || operation == CX_ALLREDUCE; }
        std::ostream & Print(std::ostream &, bool deep = false) const;
    };

  private:
    // fabric messages generated by this accelerator
    class AccelMessage : public GeneratorWorkloadMessage
    {
      public:
        AccelMessage(GeneratorWorkloadMessage::Factory *f, int src, int dst, msg_t typ = SendRequest, int dsize = 0)
          : GeneratorWorkloadMessage(f, src, dst, typ, false, dsize) {}
        AccelMessage(const AccelMessage &r, bool make_reply = false) // copy- and reply- constructor
          : GeneratorWorkloadMessage(r, make_reply) {}

        WorkloadMessagePtr Reply() { return new AccelMessage(*this, true); }
        std::ostream & Print(std::ostream &, bool deep = false) const;
    };

    // per-node barrier/collective state
    class AccelNode {
        typedef deque<WorkloadMessagePtr> msg_q_t;
        typedef boost::coroutines::asymmetric_coroutine<WorkloadMessagePtr>::pull_type coro_t;
      public:
        typedef boost::coroutines::asymmetric_coroutine<WorkloadMessagePtr>::push_type sink_t;
        typedef void (AccelNode::*collx_fptr_t)(sink_t&); // method pointer
      private:
        static const map<collx_op_t, collx_fptr_t> _op_f; // collective routines, indexed by collective op type

        CollectiveAccel &  _xl;              // collectives accelerator object
        const int          _me;              // my node ID
        msg_q_t            _net_outq;        // outgoing fabric messages
        msg_q_t            _inflight;        // inflight requests
        msg_q_t            _net_inq;         // incoming fabric messages
        msg_q_t            _net_replyq;      // reply messages
        coro_t *           _coro;            // collective algorithm coroutine
        collx_op_t         _cur_op;          // the current operation type we are performing
        int64_t            _xl_time;
      public:
        bool               has_msg() const   { return !_net_outq.empty() && GetSimTime() >= _xl_time; }
        WorkloadMessagePtr get_msg() const   { return _net_outq.front(); }
        void               next_msg()        { _net_outq.pop_front(); }
        bool               has_reply() const { return _coro->get().get() != 0; }
        WorkloadMessagePtr get_reply()       { return _coro->get(); }
        void               next()            { if (*_coro) (*_coro)(); }
        
        int get_id()                         { return _me; }
        int64_t get_xl_time()                { return _xl_time; }

        AccelNode(CollectiveAccel& xl, int id) : _xl(xl), _me(id), _coro(0), _cur_op(CX_NO_OP) {}
        void init();                         // start co-routine
        bool reqin(WorkloadMessagePtr);      // handle incoming request message
        void sendin(WorkloadMessagePtr);     // handle incoming fabric message
        void reply(WorkloadMessagePtr);      // handle reply message
      private:
        void _send_to(int dst, msg_t typ = WorkloadMessage::SendRequest, int dsize = 0); 
        void _recv(sink_t& sink, int dst);
        void _recv_multiple(sink_t& sink, int nodes);
        void _recv_replies(sink_t&);         // wait until all the replies are received
        void _wait_local_pes(sink_t&);       // wait for request messages from local PEs
        void _notify_local_pes(sink_t&);     // send completion message to local PEs 
        void _coro_f(sink_t&);               // collective algorithm top-level coroutine
        void _barrier(sink_t&);              // coroutine functions to perform a particular collective
        void _allreduce(sink_t&);
        void _bcast(sink_t&);
        int* _build_tree(int radix, int *parent, int *num_children);
      public:                                // only public since top class needs to set pointers to them:
        void _remote_barrier_linear(sink_t&);// different algorithms for the remote part of collectives
        void _remote_barrier_tree(sink_t&);
        void _remote_barrier_all2all(sink_t&);
        void _remote_barrier_dissem(sink_t&);
        void _remote_barrier_btfly(sink_t&);
        void _remote_tree_ring(sink_t&);

        void _remote_allreduce_linear(sink_t&);
        void _remote_allreduce_tree(sink_t&);
        void _remote_allreduce_ring(sink_t&);
        void _remote_allreduce_recdbl(sink_t&);
        void _remote_allreduce_rabenseifner(sink_t&);
        void _local_reduction(sink_t&, int, int, int);
        void _recv_and_reduce(sink_t&, int, int, int);

        void _remote_bcast_linear(sink_t&);
        void _remote_bcast_tree(sink_t&);
    };

    static const map<string, AccelNode::collx_fptr_t>     // collective remote sync algorithms, indexed by name
                         _str2barr, _str2red, _str2bcast;
    const AccelNode::collx_fptr_t                         // remote sync algorithm method pointers
                         _remote_barrier, _remote_allreduce, _remote_bcast;
    const int            _barrier_radix;                  // radix of the synch tree
    const int            _barrier_root;                   // node id of external synch tree root
    /*const*/ size_t     _nppn;                           // number of local PEs on this node
    vector<AccelNode>    _nodes;                          // per-node barrier/collective state
    int                  _compute_lat;

    static AccelNode::collx_fptr_t                        // get method pointer from options string
                         _opt2f(const vector<string>&, size_t, const string&, const map<string, AccelNode::collx_fptr_t>&);
    static Request*      _req_msg(WorkloadMessagePtr m)   { return m->ContentsOfType<Request>(); }
    static AccelMessage* _accel_msg(WorkloadMessagePtr m) { return dynamic_cast<AccelMessage*>(m.get()); }
    void                 _handle_reply(AccelNode& n)      { if (n.has_reply()) { _upstream->eject(n.get_reply()); } }

    WorkloadMessagePtr _get_new(int);
   
    // support for generating replies, in case Booksim does not
    const bool _gen_replies; // do we need to generate replies?
    vector<list<WorkloadMessagePtr> > _replies_pending; // replies waiting to be sent

  public:
    CollectiveAccel(int, const vector<string> &, Configuration const * const, WorkloadComponent *);
    void Init(int, Configuration const * const);
    bool test(int);
    void next(int);
    void eject(WorkloadMessagePtr);
};
